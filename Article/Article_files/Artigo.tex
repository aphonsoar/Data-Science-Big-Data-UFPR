\documentclass[twocolumn]{rbef}
\usepackage{lipsum}

\usepackage{bbm}
\usepackage{subfig}
\usepackage{pdfpages} % Para incluir a capa.

\newcommand{\1}{\mathbbm{1}}
\newcommand{\s}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\ket}{\rangle}
\newcommand{\bra}{\langle}

\newtheorem{defi}{Definição}
\newtheorem{theorem}{Teorema}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\titulocabecalho{Comportamento do cliente: estudo de caso de uma empresa prestadora de serviços de contabilidade digital}
\autorcabecalho{A. H. A. Rafael}

\numeracao{01}
\volume{01}
\numero{01}
\ano{2020}
\doi{http://dsbd.leg.ufpr.br/tcc}
% \tipodeartigo{TCC DSBD}
\tipodeartigo{Especialização em Data Science \& Big Data}
% \addtocounter{page}{566} %% \setcounter produces extra white page!!! use ===\addtocounter===
\begin{document}

\author[1]{Aphonso H. A. Rafael}\thanks{\href{emailto:aphonsoarafael@gmail.com}{aphonsoarafael@gmail.com}}

\affil[1]{Departamento de Estatística, Universidade Federal do Paraná. 
  Rua XV de Novembro, 1299, Centro, Curitiba, PR, Brasil.
}

\titulo{Comportamento do cliente: estudo de caso de uma empresa prestadora de serviços de contabilidade digital}

% \subtitulo{vazio}

% -----------------------------------------------------------------------

\includepdf[pages=-]{Capa.pdf} % Incluí a capa

\begin{primeirapagina}

  % \begin{center}
  %   \vspace{-12pt} \small{Recebido em xxx. Aceito em xxx}
  % \end{center}

\begin{abstract}
Esta pesquisa se propõe a ser um primeiro passo num projeto maior de \emph{customer success} (sucesso do cliente), numa empresa que presta serviços de contabilidade digital, é um estudo de caso a partir do atual cenário da empresa. Acredita-se que, através de dados e técnicas de \emph{machine learning}, podem ser extraídas informações e \emph{insights} importantes que direcionem projetos, ações e a tomada de decisão, implicando numa melhor prestação do serviço e a melhoria da experiência do cliente. A partir dos resultados colhidos, a empresa ganha significativo material para as seguintes análises exploratórias e novas descobertas no desenvolvimento de seus objetivos. Empresas jovens, em modelos de negócio inovadores/disruptivos, carecem de modelos ou \emph{benchmarks} de atuação, pois praticamente não há pares ou experiências similares no mercado. Sob este contexto, essa pesquisa sugere uma metodologia para análise de dados do perfil dos clientes, bem como na forma e no porque estes interagem com a empresa.
\palavraschave{contabilidade digital, perfil de cliente, atendimento, service-desk, experiência ao cliente.}

\end{abstract}

\end{primeirapagina}

\section{Introdução}

A contabilidade é uma ciência antiga e que permanece em constante evolução, com registros anteriores ao nascimento de Cristo até os dias atuais. A base da contabilidade moderna, utilizada em todas as empresas do mundo, é conhecida como o “Método das Partidas Dobradas” (SCHIMIDT, 2008)\cite{SCHMIDT}. Negra (2003, p. 1)\cite{NEGRA} destaca que “o Método das Partidas Dobradas foi exposto, oficialmente, por Luca Pacioli em 1494, em sua obra Suma de Aritmética, Geometria, Proporção e Proporcionalidade”; ainda que a autora também destaque e demonstre que o método é anterior a Pacioli, existindo registros históricos ainda anteriores ao século XVI (NEGRA, 2003)\cite{NEGRA}.
\newline\linebreak Como cita Imperatore (2017)\cite{IMPERATORE} a contabilidade é tratada como uma ciência social, cujo objetivo é estruturar um sistema de avaliação e fornecimento de informações, a fim de “[...] prover seus usuários com relatórios, demonstrações e análises de natureza econômico-financeira da empresa em determinado momento, bem como sua evolução em determinado período” (IMPERATORE, 2017, p. 12)\cite{IMPERATORE}. Ainda que evidentemente antiga, a contabilidade é um assunto técnico e pouco compreendido por quem não é, necessariamente, um estudioso da área ou frequente usuário dos seus métodos, processos e relatórios. 
\newline\linebreak Com a aceleração digital que o mundo vem enfrentando em todos os segmentos, os serviços contábeis também vem passando por muitas mudanças, seja na forma como é prestado, até mesmo em como o empresário lida com suas rotinas contábeis. É neste contexto que começam a surgir no Brasil, empresas especializadas na prestação deste serviço de maneira totalmente online, denominadas de “Contabilidade Digital”. No Brasil, a partir de 2013 este movimento passou a ganhar força com o “[...] uso da tecnologia para automatizar processos e rotinas na regularização das contas das empresas” (ISTOÉ DINHEIRO, 2020)\cite{ISTOE}.
\newline\linebreak Não apenas no contexto de prestação de serviços contábeis de maneira digital, estas empresas também têm sido importantes responsáveis no acesso e democratização à formalização de novos negócios, antes informais (sem registro, CNPJ, etc) e também ao facilitar a abertura de novas empresas; formalizando atividades de diversos tipos de empreendedores (ISTOÉ DINHEIRO, 2020)\cite{ISTOE}; sobretudo as de pequeno porte que, em 2008, eram mais de 10 milhões de empreendimentos no Brasil, sendo 98\% deles informais (EXAME, 2008)\cite{EXAME}.
\newline\linebreak Nesse cenário, esta pesquisa é um estudo de caso de uma das principais empresas de Contabilidade Digital em atuação no Brasil, que é um mercado é novo, tanto para os usuários, quanto para a própria empresa - que também tem poucos anos de existência e é vetor deste novo paradigma. Por isso, diversas questões precisam ser analisadas e discutidas, para que a empresa possa entregar uma experiência cada vez melhor aos clientes, assim como aprimorar seus processos internos, alocação de recursos, produto e a combinação de pessoas e tecnologia, para alavancar sua eficiência, crescimento e perenidade.



\section{Contextualização e motivação} \label{Sessao2}

Como citado, a contabilidade é um assunto complexo e muitas vezes subjetivo até mesmo para quem é da área. Micro e pequenos empresários, usuários dos serviços de Contabilidade Digital (e novos empreendedores) não costumam dominar o tema. Por isso, acredita-se que existem diversos tipos de empresas e negócios e, para cada um deles, dúvidas e necessidades/demandas distintas em sua jornada.
\newline\linebreak Pragmaticamente, por esta pesquisa busca-se encontrar um caminho para oferecer uma proposta de valor diferenciada aos clientes da empresa, que vá ao encontro de suas necessidades, conforme a demanda que eles possuem, ou seja, num viés de personalização da prestação do serviço conforme o tipo do cliente e o que ele busca. 
Para tal, a pesquisa foi estruturada em 3 grandes etapas: 
\newline\linebreak\textbf{1.} Conhecimento, por parte da empresa, de quem são seus clientes (quais são os tipos/perfis);  
\newline\linebreak\textbf{2.} Conhecer os tipos de demanda que seus clientes possuem; isto é: quando o cliente precisa interagir com o atendimento da empresa (service-desk); que tipo de demandas eles buscam; 
\newline\linebreak\textbf{3.} A partir dos resultados obtidos nos itens 1 e 2; analisar os perfis Vs. demandas para compreender as diferenças de comportamentos e necessidades dos clientes em sua jornada. 
Espera-se que, a partir dos resultados, a empresa possa encontrar respostas e elaborar ações, sejam de melhoria de processos, como de produto e tecnologia, para oferecer uma experiência cada vez melhor aos clientes bem como elevar sua alavancagem, escala e eficiência operacional.



\section{Metodologia e bases de dados} \label{Sessao3}

Para cada uma das etapas, foram utilizadas técnicas distintas de análise de dados, uma vez que buscavam endereçar questões diferentes.
Todos os dados utilizados são da própria companhia, extraídos pelo autor diretamente dos bancos de dados, por meio de consultas em SQL. Uma vez que aqui são tratados dados de clientes e, em alguns casos, dados internos e sigilosos, buscou-se manter a confidencialidade e análise agregada, que não permita a identificação. Quando necessário, também foram omitidos alguns dados para respeitar a confidencialidade.
\newline\linebreak\textbf{Etapa 1}: a empresa possui hoje mais de 20 mil clientes, evidentemente, não há como analisá-los ou imaginar uma prestação de serviço customizada de forma individual, por isso, o objetivo foi agrupar (clusterizar) estes clientes e identificar os perfis, semelhanças e diferenças. Uma vez que também não se sabia o número de possíveis perfis dentre essa massa de clientes, foram utilizadas técnicas de machine learning para clusterização, por meio de Agrupamento Hierárquico e análise de dendrograma. Após identificar o número de clusters pelo dendrograma, utilizou-se o \emph{K-Means} para agrupamento e nova avaliação dos resultados e análise das variâncias explicadas, a partir do números de clusters, pelo \emph{Elbow Method}. As \emph{features} consideradas para análise de agrupamento foram selecionadas pelo autor, conforme critérios negociais da empresa, ou seja, incluindo no modelo variáveis que do ponto de vista de negócio poderiam influenciar nos tipos de demandas/necessidades dos clientes, durante sua jornada.
\newline\linebreak\textbf{Etapa 2}: buscou-se compreender os tipos de demandas que os clientes possuem com os serviços prestados. Os clientes possuem dois canais para atendimento (no service-desk) com a empresa, ambos são por escrito e inteiramente registrados: i) abertura de chamados (tickets), via troca de mensagens por email e ii) abertura de atendimento instantâneo via troca de mensagens instantâneas por chat.
O service-desk é segmentado em três níveis: 1) nível 1: topo do funil de atendimentos, isso é, a entrada de todo tipo de dúvida e de cliente. Aqui são atendidas dúvidas mais simples, de usabilidade do sistema ou rotinas básicas do cliente. 2) segundo nível, onde chegam dúvidas técnicas mais complexas, como questões legais, tributárias, ou para tipos de negócio/clientes específicos, que precisam de análise profissional detalhada e tomada de decisão. 3) o nível 3, por fim, refere-se a prestação de serviços, como alterações de documentos, reabertura de balanços, consultoria paga (legal e tributária), ajustes contábeis ou demais atividades que extrapolam os serviços básicos cobertos no contrato. Os atendimentos dos níveis Avançado e Premium \textbf{(cfe. seção 4.2.2)} agregam os níveis 1 e 2 numa única célula de atendimento, porém para os clientes que aderem a estes planos.
Desta forma, foi selecionada uma base de dados contendo os atendimentos realizados nos níveis 1, 2, 3, Avançado e Premium, durante o período de sete meses (janeiro a julho de 2020). O total de itens analisados (tickets + chats) foi de aproximadamente 77 mil atendimentos e, nesta base, foram utilizadas técnicas de machine learning para mineração de texto (MinTex) / processamento de linguagem natural \emph{(Natural Language Processing - NLP)}. Em sequência, foram treinados e testados modelos de classificação a partir de uma base rotulada de 383 itens - que foi classificada pela empresa para esta pesquisa - categorizando estes atendimentos entre as opções i) dúvida; ii) problema; iii) solicitação e, por fim, preditos/classificados os atendimentos.
\newline\linebreak\textbf{Etapa 3}: a partir dos resultados dos itens 1 e 2; foram realizadas análises exploratórias e  estatísticas descritivas. Na sequência, os resultados dos itens 1 e 2 foram combinados para identificar e descrever os perfis e tipos de demandas de cada um deles, com o objetivo de prover então à empresa, os insights propostos nesta pesquisa.
Devido ao grande volume de dados, o processamento dos algoritmos e análises foram realizados em uma nuvem fornecida pela própria empresa, em ambiente GCP (Google Cloud Platform). A configuração da máquina foi uma VM standard do GCP (n1-highmem-16); cuja infraestrutura é composta de 16 vCPUs com 104 GB de memória e sistema operacional Debian.
As modelagens, técnicas estatísticas e de machine learning foram executadas na linguagem Python, versão base 3.8; por meio das principais bibliotecas utilizadas para essas finalidades, tais como Scikit-Learn, SciPy, NLTK e as clássicas bibliotecas de manipulação e visualização de dados como Pandas, Numpy, Matplotlib e Seaborn.
Os códigos, assim como os requerimentos com o detalhamento das bibliotecas e versões (requirements.txt), estão disponibilizados no \href{https://github.com/aphonsoar/Data-Science-Big-Data-UFPR}{\emph{Github}}.




\section{Experimentos e análise dos resultados} \label{Sessao4}
\subsection{Clusterização de clientes} \label{Sessao4.1}

O dataset com dados dos clientes foi elaborado de maneira em que cada linha representa um CNPJ (ou seja, uma empresa) e as colunas são as features (variáveis) para clusterização. Originalmente, ele possuía além da chave (CNPJ) outras 17 variáveis (features), sendo 10 numéricas e 7 categóricas. Uma vez que “não há” como realizar agrupamento por variáveis categóricas, foi necessária uma etapa de pré-processamento de dados, transformando estas variáveis categóricas em numéricas (SARKAR, 2019)\cite{SARKAR}. Convencionalmente, duas técnicas são principalmente utilizadas para tal: i) Label Encoder e ii) One Hot Encoding (SHAIKH, 2018)\cite{SHAIKH}:
%--------------------------------------------------------------------------------
\newline\linebreak\textbf{i) Label Encoder:} define as variáveis categóricas em números inteiros entre 0 e N (n\_classes -1), onde N é o número de classes distintas. Se a classe se repete, o mesmo número inteiro é atribuído a todas as repetições. \textbf{Limitações:} como os valores das variáveis categóricas passam a ser de 0 a N, isso cria uma impressão de relacionamento e continuidade para o modelo, ou seja, que a classe 1 é menor que a 2, a 2, menor que a três e assim sucessivamente: 1 < 2 < 3 < N (relação ordinal).
%--------------------------------------------------------------------------------
\linebreak\textbf{ii) One Hot Encoding:} segrega as variáveis categóricas entre seus valores, atribuindo um valor binário, ou seja: será criada uma coluna para cada classe, de cada variável categórica, é definido o valor zero ou um (0 ou 1) para essa coluna, sendo 1 verdadeiro e 0 falso. \textbf{Limitações:} como cada classe, de cada variável categórica, é transformada numa coluna no dataset, isso pode aumentar consideravelmente o número de atributos do dataset (colunas na base), fazendo com que se amplie o risco de \emph{Maldição da Dimensionalidade}.
%--------------------------------------------------------------------------------
\newline\linebreak Após testes realizados com ambos os métodos, optou-se, finalmente, pelo segundo: (One Hot Encoding). Isto, pois o número de colunas não aumentou de forma a impactar nos graus de liberdade ou causar Maldição da Dimensionalidade; além de que o outro método criaria a relação ordinal entre os valores das variáveis, influenciando na clusterização. Com isso, o dataset passou ter 49 \emph{features}, sendo as mesmas 10 numéricas e 39 categóricas. Na figura 1 é possível verificar a composição do dataset. Em \emph{vermelho} estão destacadas as variáveis categóricas e em \emph{azul} as numéricas. Apenas uma das colunas apresentou \emph{missing values}: [Idade\_media\_socios]. Estes registros eram cerca de 9\% dos itens e foram ignorados na clusterização.
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \includegraphics[scale=0.5]{1. Dataset clientes - variaveis.png}
  \caption{Dataset clientes: features consideradas}
  \label{fig1}
\end{figure}
%--------------------------------------------------------------------------------
\newline\linebreak Formalmente, o método do Agrupamento Hierárquico parte da base de dados, isto é, todos os N itens do dataset (todos os clientes) formando N \emph{clusters}, onde N é o número de clientes. A partir dos \emph{N clusters}, ele encontra os 2 pontos mais próximos entre si, através do cálculo da distância euclidiana (Nx;Ny) e cria um novo \emph{clusters} agregando estes 2, num processo aglomerativo. O processo é seguido sucessivamente até que todos os valores se tornem 1 único \emph{clusters}. Este processo é representado, portanto, pelo gráfico de dendrograma, onde é possível visualizar os níveis de cada nó da agregação e definir qual o melhor número de \emph{k}, onde \emph{k = nº clusters}. 
Neste agrupamento foi utilizado o Método de Ward, que se consiste num “[...] procedimento de agrupamento hierárquico no qual a medida de similaridade usada para juntar agrupamentos é calculada como a soma de quadrados entre os dois agrupamentos feita sobre todas as variáveis” (SEIDEL et al., 2008, p. 10)\cite{SEIDEL}. O método é representado pela equação abaixo e utiliza um algoritmo de minimização variâncias, que “[...] tende a resultar em agrupamentos de tamanhos aproximadamente iguais” (SEIDEL et al., 2008, p. 10)\cite{SEIDEL}.
%--------------------------------------------------------------------------------
\begin{align*}
d(u, v) = \sqrt{\frac{|v| + |s|}{T} d(v, s)^2 + \frac{|v| + |t|}{T} d(v, t)^2 - \frac{|v|}{T} d(s, t)^2}
\end{align*}
%--------------------------------------------------------------------------------
Na equação, \emph{u} é o novo cluster aglomerado a partir dos clusters \emph{s} e \emph{t}. Por sua vez, \emph{v} é um cluster não utilizado na floresta \emph{T = |v| + |s| + |t|}, e o fator de multiplicação \emph{|*|} é a cardinalidade desses argumentos. Isso também é conhecido como algoritmo incremental (SCIPY, 2020)\cite{SCIPY}.
%--------------------------------------------------------------------------------
\newline\linebreak Os resultados obtidos podem ser vistos nas figuras 2 e 3 (Dendrograma e Elbow Method Chart):
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \includegraphics[scale=0.22]{2. Dendrograma Clientes 7 clusteres.png}
  \caption{Dendrograma com últimos 50 nós}
  \label{fig2}
\end{figure}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \includegraphics[scale=0.22]{3. Elbow - Clientes 7 clusteres.png}
  \caption{Gráfico do Elbow Method cortado em 7 clusters}
  \label{fig3}
\end{figure}
%--------------------------------------------------------------------------------
\newline\linebreak Conforme análise do Dendrograma e também pelo ponto de inflexão do gráfico do \emph{Elbow Method}, definiu-se portanto, a clusterização dos clientes em 7 grupos, distribuídos conforme figura 4:
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \includegraphics[scale=0.26]{4. Distribuição_CNPJs_clusters_7.png}
  \caption{Distribuição dos clientes entre os clusters}
  \label{fig4}
\end{figure}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\subsection{Classificação e predição dos atendimentos} \label{Sessao4.2}
\subsubsection{Pré-processamento e análise exploratória} \label{Sessao4.2.1}
Para análise dos atendimentos, foram construídos inicialmente dois datasets: i) tickets (emails) e ii) chats. Para ambos, todo o histórico de troca de mensagens entre o operador e o cliente foram concatenados em uma única \emph{string} e em seguida os datasets foram unidos em uma base única. Quando se trabalha com MinTex e NLP também é importante realizar pré-processamento dos dados para que as \emph{strings} / sentenças possam oferecer melhores condições preditoras (BOEHMKE, 2020)\cite{BOEHMKE}. Este pré-processamento foi realizado nas seguintes etapas:
%--------------------------------------------------------------------------------
\newline\linebreak\textbf{i) normalização do texto:} a) palavras convertidas para lowercase (uma vez que os algoritmos são case sensitive); b) removidos os símbolos e pontuações; b) removidos espaços vazios (em branco) em excesso e; d) removidos os números (0 a 9) das strings.
%--------------------------------------------------------------------------------
\newline\linebreak\textbf{ii) \emph{stopwords}:} são palavras que não possuem significado sintático e não colaboram com informações relevantes numa sentença, seja no sentido ou compreensão da frase, como também poderão atrapalhar ou confundir o algoritmo. São palavras como: “e”, “ou”, “para”, “de” e similares, preposições e termos de ligamento; por isso é recomendado sua remoção (SARKAR, 2019)\cite{SARKAR}. A biblioteca NLTK possui uma lista de \emph{stopwords} em diversos idiomas, entre elas o Português. Foi utilizada esta lista \emph{default} com o incremento de mais algumas centenas de palavras pelo autor, conforme contexto do texto e da análise, totalizando 557 \emph{stopwords}.
%--------------------------------------------------------------------------------
\newline\linebreak Uma vez que o volume de atendimentos analisados está na casa das dezenas de milhares, é evidente imaginar que existe uma infinidade de palavras distintas utilizadas em todas estas trocas de mensagens, por isso, inicialmente foi feita uma análise exploratória das palavras usadas, onde insights importantes foram extraídos.
%--------------------------------------------------------------------------------
\newline\linebreak Por meio da da biblioteca NLTK, foi utilizado um algoritmo para “tokenizar” cada palavra que aparece em cada um dos atendimentos, isto é, fazer uma uma contagem de quantas vezes cada palavra se repete em todas as sentenças. A partir disso, podemos analisar sua frequência e distribuição:
%--------------------------------------------------------------------------------
\begin{table}[!htb]
\caption{Frequência das primeiras 10 palavras}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c*{5}{>{$}c<{$}}}
& \text{\textbf{Palavra}} & \text{\textbf{Frequência}} & \text{\textbf{Percentual}} & \text{\textbf{Percentual acumulado}} \\
1 & empresa & 108.261 & 0,01133 & 0.01133 \\
2 & labore & 88.762 & 0,00929 & 0.02062 \\
3 & valor & 85.508 & 0,00895 & 0.02957 \\
4 & contabilidade & 76.612 & 0,00802 & 0.03759 \\
5 & pró & 69.460 & 0,00727 & 0.04487 \\
6 & sucesso & 65.750 & 0,00688 & 0.05175 \\
7 & plataforma & 65.461 & 0,00685 & 0.05860 \\
8 & mês & 60.816 & 0,00636 & 0.06497 \\
9 & pagamento & 60.387 & 0,00632 & 0.07129 \\
10 & nota & 55.804 & 0,00584 & 0.07713 \\
\end{tabular}}
\end{table}
%--------------------------------------------------------------------------------
\newline\linebreak Na tabela 1, constam as primeiras 10 palavras que mais apareceram nos textos dos atendimentos, verifica-se que a palavra que mais apareceu foi “empresa” com frequência de 108.261. Sozinha, ela corresponde a mais de 1\% de todas as palavras ditas. Também podemos notar que as primeiras 10 palavras que mais se repetem, representavam mais de 7\% de todas as palavras escritas em todos os atendimentos. Indo um pouco mais além nesta análise exploratória, verificou-se que no total foram citadas 95.227 palavras distintas, porém, destas, apenas 5.654 já representavam mais de 95\% de todas as palavras ditas (vide figuras 6 e 7). Ainda, 52.040 palavras (54\% do total) apareceram somente uma única vez, ou seja, não teriam nenhum poder preditor (considerando apenas uma análise de pesos e frequências). As figuras 5 e 6 demonstram o histograma e o \% acumulado das primeiras 1.000 e 5.000 palavras com maior frequência:
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \includegraphics[scale=0.23]{5. Histograma das 1000 primeiras palavras.png}
  \caption{Histograma das primeiras 1.000 palavras}
  \label{fig5}
\end{figure}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \includegraphics[scale=0.23]{6. Histograma das 5000 primeiras palavras.png}
  \caption{Histograma das primeiras 5.000 palavras}
  \label{fig6}
\end{figure}
%--------------------------------------------------------------------------------
\newline\linebreak Os histogramas evidenciam uma cauda muito longa, assim como sugerem que não há necessidade (tampouco é recomendado) utilizar-se de toda a base de palavras, uma vez que isso pode prejudicar o treinamento dos algoritmos com palavras com pouca ou nenhuma relevância no processo. Considerou-se, portanto somente as 5.654 palavras que representam 95\% de toda a frequência dos textos, com objetivo de reduzir dimensionalidade e ruídos que pudessem prejudicar o modelo.
%--------------------------------------------------------------------------------
\subsection{Classificação dos atendimentos} \label{Sessao4.3}
%--------------------------------------------------------------------------------
Realizado o pré-processamento e tratamentos, finalmente pode-se avançar para a classificação. O objetivo era compreender a natureza dos atendimentos para classificá-los entre \textbf{i) dúvida}; \textbf{ii) problema}; \textbf{iii) solicitação}. Essas três classes foram definidas pelas áreas de negócio da empresa, porém a análise ainda nunca havia sido feita, isto é, não existia uma base rotulada de atendimentos para treinarmos os modelos. A partir dos cerca de 77 mil atendimentos, portanto, foi retirada uma amostra de 383 itens para classificação, o que representa um intervalo de confiança de 95\% com margem de erro de 5\%. Estes 383 itens foram rotulados conforme tabela 2 e figura 7:
%--------------------------------------------------------------------------------
\begin{table}[!htb]
\caption{Amostra dos atendimentos}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c*{5}{>{$}c<{$}}}
\text{\textbf{Tipo}} & \text{\textbf{Frente atendimento}} & \text{\textbf{Qtde}} & \text{\textbf{Percentual}} & \text{\textbf{Qtde amostra}} \\
Chat & Engajamento N1 & 31.569 & 41,15\% & 158\\
Ticket & Serviço N2 & 20.893 & 27,25\% & 104\\
Ticket & Serviço N1 & 14.917 & 19,46\% & 75\\
Ticket & N3 & 4.823 & 6,29\% & 24\\
Ticket & Avançado & 2.679 & 3,49\% & 13\\
Ticket & Premium & 1.811 & 2,36\% & 9\\
\end{tabular}}
\end{table}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \centering \includegraphics[scale=0.7]{7. Amostra classificada atendimentos.png}
  \caption{Amostra dos atendimentos classificada}
  \label{fig7}
\end{figure}
%--------------------------------------------------------------------------------
\newline\linebreak Para classificação, a amostra foi dividida numa proporção de 70\% treino e 30\% teste e quatro algoritmos foram executados e comparados para identificar o que apresentasse os melhores resultados.
Algumas métricas de validação são comuns para analisar a performance de modelos de classificação, como i) acurácia / accuracy; ii) valor preditivo negativo; iii) precisão; iv) recall; v) especificidade; vi) F1-score e vii) área sob a curva ROC; (SCUDILIO, 2020)\cite{SCUDILIO}. Essas métricas são baseadas na matriz de confusão, a partir dos acertos, erros, falsos positivos e falsos negativos do modelo onde, dependendo do objetivo a ser alcançado, cada uma delas contribui de forma diferente na avaliação dos resultados. Uma vez que no caso em questão, buscou-se apenas identificar se o modelo previu corretamente as classes, os falsos positivos e falsos negativos possuem o mesmo peso e, por esse motivo, a métrica de acurácia (accuracy) é suficiente para interpretação dos resultados. A tabela 3 apresenta os resultados da acurácia dos modelos em seu estado de parâmetros \emph{default}:
%--------------------------------------------------------------------------------
\begin{table}[!htb]
\caption{Classificadores: parâmetros default}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c*{5}{>{$}c<{$}}}
\text{\textbf{Algoritmo}} & \text{\textbf{Train (param default)}} & \text{\textbf{Test (param default)}} \\
Multinomial Naive Bayes Classifier & 60,82\% & 45,69\% \\
Stochastic Gradient Descent Classifier & 100,00\% & 56,90\% \\
C-Support Vector Classification & 97,76\% & 47,41\% \\
Random Forest Classifier & 100,00\% & 50,00\% \\
\end{tabular}}
\end{table}
%--------------------------------------------------------------------------------
\newline\linebreak As colunas “Train” e “Test” (param default) representam a acurácia do modelo antes da otimização dos hiperparâmetros (model tuning). Para otimização, foram levantadas as possibilidades de combinação de parâmetros de cada um dos algoritmos, que fizessem sentido com o objetivo da análise, e criados pipelines para testes com esta seleção. A partir disso, portanto, foram usados métodos computacionais para identificar a melhor combinação dos parâmetros, isto é, aqueles que permitem um melhor ajuste e generalização do modelo aos dados.
\newline\linebreak Os métodos utilizados são da Scikit-Learn e consistem em testar, exaustivamente, todas as combinações possíveis dos parâmetros definidos até que a métrica considerada como o indicador de performance (no caso a acurácia), apresente o maior valor nas bases de treino e teste. Por fim, o método também realiza a validação cruzada nas bases de treino e teste conforme a definição de \emph{folds}. Na tabela 4, constam os os mesmos modelos após a otimização dos parâmetros e validação cruzada com 5 \emph{folds}:
%--------------------------------------------------------------------------------
\begin{table}[!htb]
\caption{Classificadores: hiperparâmetros otimizados}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c*{5}{>{$}c<{$}}}
\text{\textbf{Algoritmo}} & \text{\textbf{Test (best param) cross-validation}} & \text{\textbf{Classificação}} \\
Multinomial Naive Bayes Classifier & 60,46\% & 1 \\
Stochastic Gradient Descent Classifier & 63,05\% & 3 \\
C-Support Vector Classification & 58,95\% & 4 \\
Random Forest Classifier & 63,45\% & 2 \\
\end{tabular}}
\end{table}
%--------------------------------------------------------------------------------
\newline\linebreak O classificador escolhido foi o \emph{Multinomial Naive Bayes (NB)}, pois foi o algoritmo que ficou mais equilibrado aos dados, com menor distância de acurácia entre treino e teste, o que não aparenta \emph{overfit}, além de todas as classes terem apresentado resultado similar e satisfatório na predição da classificação (entre treino e teste). Sarkar (2019, p. 298)\cite{SARKAR} cita que “[...] o algoritmo Naive Bayes é usado especificamente para tarefas de predição e classificação tarefas onde temos mais de duas classes” (traduzido pelo autor).  Na figura 8, podemos ver a matriz de confusão do classificador:
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \centering \includegraphics[scale=0.5]{8. Matriz_confusao_NB_normalizada.png}
  \caption{Matriz de confusão normalizada: Naive Bayes Classifier}
  \label{fig8}
\end{figure}
%--------------------------------------------------------------------------------
\newline\linebreak Conforme figura 8, a acurácia final do classificador ficou em 60,46\%. Apesar de não ser ainda um valor tão alto, esta análise não se propõe a ser exaustiva, isto é, ainda existem diversos fatores a serem aprimorados para uso na empresa; porém isso também não impede a continuidade do estudo para que seja possível extrair insights iniciais numa primeira observação. Dentre as ações a serem tomadas para melhoria do classificador, provavelmente a aumentar a amostra rotulada para ter uma maior base de treino seja a mais importante como \emph{step 1}.
%--------------------------------------------------------------------------------
\subsection{Combinação dos resultados da clusterização e classificação e insights iniciais} \label{Sessao4.4}
%--------------------------------------------------------------------------------
Combinando os resultados dos clientes clusterizados e dos atendimentos classificados, foi possível identificar comportamentos interessantes. Nas figuras 9 e 10, podemos verificar que, ainda que a distribuição dos motivos seja unânime entre os grupos - ou seja, duvidas com maior participação, seguido por problema e solicitação - nas dúvidas, o grupo 1 se destaca de forma significativa para mais, enquanto os os grupos 2 e 5 para menos. Interessante notar que o grupo 1 também é o mais volumoso em número de clientes; ao passo em que os grupos 2 e 5 estão entre os menos volumosos (vide figura 4).
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \centering \includegraphics[scale=0.35]{9. Classificação atendimentos (7 clusters) - chart.png}
  \caption{Distribuição das classes pelos grupos}
  \label{fig9}
\end{figure}
%--------------------------------------------------------------------------------
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \centering \includegraphics[scale=0.7]{10. Classificação atendimentos (7 clusters) - table.png}
  \caption{Mapa de calor das classes pelos grupos}
  \label{fig10}
\end{figure}
%--------------------------------------------------------------------------------
\newline\linebreak Já na figura 11, podemos notar que os grupos 1, 4 e 6 apresentam uma taxa de contato significativamente menor do que os demais, ou seja, geram menos demanda no atendimento para a companhia. Também é interessante observar que os grupos com maiores taxas de contato (2, 3, 5 e 7) também apresentam o volume de “solicitações” maior que os demais.
%--------------------------------------------------------------------------------
\begin{figure}[!htb]
  \centering \includegraphics[scale=0.6]{11. Concat rate (7 clusters).png}
  \caption{Taxa de contato (contact rate) por grupo}
  \label{fig11}
\end{figure}
%--------------------------------------------------------------------------------
\section{Conclusão} \label{Sessao5}
A partir da análise preliminar dos resultados, realizada no item \ref{Sessao4.4}; fica evidente que existe distinção entre os tipos de clientes da empresa e na forma como estes se comportam e interagem os com ela, o que era uma das questões centrais a serem respondidas por meio deste estudo.
Por meio dos resultados da etapa 1 (clusterização - \ref{Sessao4.1}), foi possível verificar que a maioria das variáveis consideradas apresentaram comportamentos diferentes para cada um dos 7 grupos, o que implica na inferência de que foram úteis na separação desses clientes, e permitindo compreender o que é relevante nessa separação, bem como identificar e traçar perfis para os grupos e os clientes que fazem parte deles.
Já na etapa \ref{Sessao4.2.1}, a partir da classificação dos atendimentos, verificou-se que as dúvidas são o maior motivo de contato dos clientes com a empresa, sendo cerca de metade deles. As outras duas categorias (problema e solicitação) se dividem em duas partes similares a outra metade. Ou seja, fica evidente que para reduzir o volume de atendimentos demandados, o foco inicial da empresa deve ser na redução das dúvidas. Importante considerar também, no entanto, que essa proporção varia significativamente entre os grupos, o que também era uma questão central a ser compreendida.
No entanto, esta pesquisa não é exaustiva, tampouco se propôs a esgotar o assunto, mas sim em dar um primeiro passo, obter insights e identificar critérios que permitissem compreender os perfis de clientes a relação deles com os tipos de demandas que fazem no atendimento \emph{(service-desk)}. Isto é, criar uma metodologia inicial de análise para que a empresa possa entender a jornada de seus clientes e elaborar ações, seja de processos ou tecnologia, que melhorem a experiência.
A partir destes primeiros insights, cabe como próximos passos uma extensa análise exploratória dos resultados, considerando as demais variáveis utilizadas na clusterização dos clientes, e em conjunto com as áreas de negócio da empresa, para melhor interpretação e assim enfim, colaborar na nos planos de ação a serem formulados e no desenvolvimento do negócio, seja na experiência do cliente, ou na melhora de eficiência da companhia, via alavancagem operacional do seu time de atendimento, com automação de rotinas repetitivas ou que agregam pouco valor a cia e/ou ao cliente.
%--------------------------------------------------------------------------------

\begin{thebibliography}{99}
  \bibliographystyle{unsrt}

\bibitem{SCHMIDT} SCHMIDT, Jose Luiz dos Santos e Paulo. 
    \textit{História da Contabilidade: foco na evolução das escolas do pensamento contábil.} Santos: Atlas, 2008. 176 p.

\bibitem{NEGRA} NEGRA, Elizabete Marinho Serra.
    \textit{Evidências das partidas dobradas através da Matemática na Mesopotâmia.} Revista do CRC/PR, Curitiba, 2003.

\bibitem{IMPERATORE} IMPERATORE, Simone Loureiro Brum.
    \textit{Fundamentos da Contabilidade.} Curitiba: Intersaberes, 2017. 173 p.

\bibitem{ISTOE} ISTOÉ DINHEIRO.
    \textit{Tecnologia - Contabilidade Digital: escritórios contábeis on-line ganham espaço com modelo de startup e promessa de custos menores.} 2020.

\bibitem{EXAME} EXAME.
    \textit{Informalidade atinge 98\% das pequenas empresas.} 2008.

\bibitem{SHAIKH} SHAIKH, Raheel.
    \textit{Choosing the right Encoding method-Label vs OneHot Encoder.} Towards Data Science, 2018.

\bibitem{SCIPY} Scipy.
    \textit{Cluster Hierarchy Linkage.} Scipy Python Library, 2020.

\bibitem{SARKAR} SARKAR, Dipanjan.
    \textit{Text Analytics with Python: a practitioner's guide to natural language processing.} 2. ed. Bangalore, Karnataka, India: Apress, 2019. 668 p.

\bibitem{BOEHMKE} BOEHMKE, Bradley.
    \textit{Business Analytics R Programming Guide: creating text features with bag-of-words, n-grams, parts-of-speach and more.} University of Cincinnati, 2020.

\bibitem{SCUDILIO} SCUDILIO, Juliana.
    \textit{Qual a melhor métrica para avaliar os modelos de Machine Learning?} Flai Inteligência Artificial, 2020.

\bibitem{SEIDEL} SEIDEL, Enio Júnior et al.
    \textit{Comparação entre o método Ward e o método K-médias no agrupamento de produtores de leite.} Santa Maria, 2008.

\end{thebibliography}
\end{document}
